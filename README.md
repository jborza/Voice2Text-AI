# Voice2Text AI

<img src="logo.png" alt="Voice2Text AI Logo" width="200"/>

## Application Screenshot

<img src="Screenshot at 2025-11-03 01-22-46.png" alt="Voice2Text AI Application Screenshot" width="600"/>

## Downloads

- [Windows](https://github.com/crhy/Voice2Text-AI/releases/download/v0.2/Voice2Text.exe)
- [macOS](https://github.com/crhy/Voice2Text-AI/releases/download/v0.2/Voice2Text)
- Linux: Flatpak coming soon to [Flathub](https://flathub.org/apps/com.voice2text.app)

A cross-platform Python application that transcribes voice input using GPU-accelerated Whisper, sends text to local Ollama AI models for intelligent responses, and reads the output aloud with text-to-speech.

## Features

- ğŸ™ï¸ Real-time voice transcription with OpenAI Whisper (GPU-accelerated)
- ğŸ¤– AI chat integration with local Ollama models
- ğŸ”Š Text-to-speech output
- ğŸ¨ Modern dark gradient UI
- ğŸ“¦ Cross-platform executables (Windows/macOS/Linux)
- ğŸ–¥ï¸ Flatpak available on Flathub



## Requirements

- Python 3.8+
- Ollama running locally (http://localhost:11434)
- Microphone access
- For GPU acceleration: CUDA-compatible GPU (optional)

## Installation

### Option 1: Download Executable (Windows/macOS)
Download the latest release for your platform from [GitHub Releases](https://github.com/crhy/Voice2Text-AI/releases):
- **Windows**: `Voice2Text.exe`
- **macOS**: `Voice2Text`

### Option 2: Linux Installer (Recommended for Linux)
For Linux users who don't have Ollama installed, download and run the installer script to set up Ollama and the model automatically:
```bash
wget https://github.com/crhy/Voice2Text-AI/releases/download/v0.2/install.sh
chmod +x install.sh
./install.sh
```
This installs Ollama and the required model. For the app, install the flatpak from Flathub once available:
```bash
flatpak install flathub com.voice2text.app
```

### Option 3: From Source
1. Clone the repository:
    ```bash
    git clone https://github.com/crhy/Voice2Text-AI.git
    cd Voice2Text-AI
    ```

2. Create a virtual environment:
    ```bash
    python3 -m venv venv
    ```

3. Activate the virtual environment:
    ```bash
    source venv/bin/activate
    ```

4. Install dependencies:
    ```bash
    pip install -r requirements.txt
    ```

5. Ensure Ollama is installed and running with models available.

## Usage

### Running the App
- **Executable**: Double-click the downloaded executable
- **From Source**: 
  ```bash
  source venv/bin/activate
  python voice_app.py
  ```

### Interface Guide
1. **Model Selection**: Choose your preferred Ollama model from the dropdown
2. **Voice Input**: Click "Start Recording" to begin voice transcription
3. **Speak**: Talk clearly into your microphone
4. **Stop**: Click "Stop Recording" when finished
5. **Edit Text**: Review and edit the transcribed text in the input area
6. **Send to AI**: Click "Send to AI" to get Ollama's response
7. **Listen**: The response will be displayed and spoken aloud

The app features a modern dark UI with real-time status indicators and remembers your settings between sessions.

## Notes

- Speech recognition uses OpenAI Whisper (runs locally, internet required for initial model download)
- Text-to-speech uses Google TTS (requires internet)
- GPU acceleration available for faster Whisper transcription
- Settings are automatically saved
- Works offline once models are downloaded

## Troubleshooting

- **Microphone Issues**: Run `python test_mic.py` to test audio input
- **Ollama Connection**: Ensure Ollama is running at http://localhost:11434
- **GPU Support**: Check CUDA installation with `nvidia-smi`
- **Permissions**: Grant microphone access to the application
- **Model Download**: First run may take time to download Whisper models

## Project Structure

```
OpenCode/
â”œâ”€â”€ Voice2Text/
â”‚   â”œâ”€â”€ public/
â”‚   â”‚   â”œâ”€â”€ electron.js
â”‚   â”‚   â”œâ”€â”€ favicon.ico
â”‚   â”‚   â””â”€â”€ index.html
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ Components/
â”‚   â”‚   â”‚   â”œâ”€â”€ Details/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Details.css
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Details.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ Editor/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ assets/
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ star.png
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Editor.css
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Editor.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ Navbar/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Navbar.css
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Navbar.jsx
â”‚   â”‚   â”‚   â””â”€â”€ Options/
â”‚   â”‚   â”‚       â”œâ”€â”€ assets/
â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ headset.png
â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ headset_on.png
â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ star.png
â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ trash.png
â”‚   â”‚   â”‚       â”‚   â””â”€â”€ volume.png
â”‚   â”‚   â”‚       â”œâ”€â”€ Options.css
â”‚   â”‚   â”‚       â””â”€â”€ Options.jsx
â”‚   â”‚   â”œâ”€â”€ App.css
â”‚   â”‚   â”œâ”€â”€ App.js
â”‚   â”‚   â”œâ”€â”€ index.css
â”‚   â”‚   â””â”€â”€ index.js
â”‚   â”œâ”€â”€ .gitattributes
â”‚   â”œâ”€â”€ .gitignore
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ com.voice2text.app.desktop
â”‚   â”œâ”€â”€ com.voice2text.app.metainfo.xml
â”‚   â”œâ”€â”€ com.voice2text.app.yml
â”‚   â”œâ”€â”€ electron.js
â”‚   â”œâ”€â”€ package-lock.json
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ run_voice_app.sh
â”‚   â”œâ”€â”€ test_whisper.py
â”‚   â”œâ”€â”€ voice_app.py
â”‚   â””â”€â”€ voice_config.json
â”œâ”€â”€ Voice2Text-Windows/
â”‚   â””â”€â”€ Voice2Text.exe
â”œâ”€â”€ Voice2Text-macOS/
â”‚   â””â”€â”€ Voice2Text
â”œâ”€â”€ flathub-repo/
â”‚   â””â”€â”€ com.voice2text.app/
â”‚       â””â”€â”€ com.voice2text.app.yml
â”œâ”€â”€ voice_env/
â”‚   â””â”€â”€ (Python virtual environment files)
â”œâ”€â”€ README.md
â”œâ”€â”€ VOICE_README.md
â”œâ”€â”€ buildozer.spec
â”œâ”€â”€ config.json
â”œâ”€â”€ main.py
â”œâ”€â”€ opencode.json
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ test_voice.py
â”œâ”€â”€ test_whisper.py
â”œâ”€â”€ voice_app.desktop
â”œâ”€â”€ voice_app.py
â”œâ”€â”€ voice_app.spec
â”œâ”€â”€ voice_app_kivy.py
â”œâ”€â”€ voice_config.json
â””â”€â”€ voice_to_opencode.py
```

## Contributing

Contributions welcome! Please submit issues and pull requests on GitHub.